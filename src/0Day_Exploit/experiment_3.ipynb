{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Movement: Periodic Defender with CVE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Cell is necessary or else my IDE will start running cells simultaneously which\n",
      "      then leads to issues with unfinished imports\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"This Cell is necessary or else my IDE will start running cells simultaneously which\n",
    "      then leads to issues with unfinished imports\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current settings: image_mode=False, debug_mode=False, RUN_BASELINE_ONLY=False\n"
     ]
    }
   ],
   "source": [
    "# Check if variables exist, otherwise default to False\n",
    "image_mode = globals().get('image_mode', False)\n",
    "debug_mode = globals().get('debug_mode', False)\n",
    "RUN_BASELINE_ONLY = globals().get('RUN_BASELINE_ONLY', False)\n",
    "\n",
    "# If you are NOT using \"extrapolate.ipynb\" uncomment any you want to set to true\n",
    "# however remember to re-comment them out later because otherwise extrapolate will be ignored\n",
    "#image_mode = True\n",
    "#debug_mode = True\n",
    "#RUN_BASELINE_ONLY = True\n",
    "\n",
    "print(f\"Current settings: image_mode={image_mode}, debug_mode={debug_mode}, RUN_BASELINE_ONLY={RUN_BASELINE_ONLY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name as a variable for easy modification\n",
    "experiment_name = \"experiment_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up main logger (original)\n",
    "log_path = os.path.join(os.getcwd(), f'{experiment_name}.log')\n",
    "if os.path.exists(log_path):\n",
    "    os.remove(log_path)\n",
    "logger = logging.getLogger()\n",
    "handler = logging.FileHandler(log_path, mode='w')\n",
    "handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(f'[1] \"{experiment_name}.log\"')\n",
    "logger.info(f'[1] \"{datetime.now().strftime(\"%a %b %d %H:%M:%S %Y\")}\"')\n",
    "\n",
    "# Set up subgraph logger if not RUN_BASELINE_ONLY:\n",
    "if not RUN_BASELINE_ONLY:\n",
    "    subgraph_log_path = os.path.join(os.getcwd(), f'sub_{experiment_name}.log')\n",
    "    if os.path.exists(subgraph_log_path):\n",
    "        os.remove(subgraph_log_path)\n",
    "    subgraph_handler = logging.FileHandler(subgraph_log_path, mode='w')\n",
    "    subgraph_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "    # Don't add to logger yet\n",
    "\n",
    "# Function to switch between loggers\n",
    "def switch_logger(use_subgraph_logger=False):\n",
    "    # Remove all handlers\n",
    "    for hdlr in logger.handlers[:]:\n",
    "        logger.removeHandler(hdlr)\n",
    "        \n",
    "    # Add appropriate handler\n",
    "    if use_subgraph_logger:\n",
    "        logger.addHandler(subgraph_handler)\n",
    "        logger.info(f'[1] \"sub_{experiment_name}.log\"')\n",
    "        logger.info(f'[1] \"{datetime.now().strftime(\"%a %b %d %H:%M:%S %Y\")}\"')\n",
    "    else:\n",
    "        logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "# This is the default weight value we will insert any time we we \n",
    "# need to insert a weight value for a trivial edge in the graph\n",
    "# Why 0 ?  Because we have \"hardness = exnp(-w)\" so we get hardness = 1 for w = 0\n",
    "# hardness = 1 means that edge is trivial to traverse\n",
    "DEFAULT_WEIGHT_VALUE = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run attack_graph_MIR100.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create subsgraph for defender concluded\n"
     ]
    }
   ],
   "source": [
    "%run create_subgraphs.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploit-Dependent Probability Distribution Explanations\n",
    "#### Base Formula\n",
    "This distribution models the probability of completing exactly n exploits and failing on the (n+1)-st in a series of exploits with varying difficulty:\n",
    "\\begin{equation}\n",
    "f_N(n) = (1 - q(e_{n+1})) \\cdot \\prod_{k=1}^{n} q(e_k)\n",
    "\\end{equation}\n",
    "Where $q(e_k)$ is the probability of successfully executing exploit $e_k$ within a unit of time.\n",
    "\n",
    "#### Definition of \"q(e)\" in this Specific Case\n",
    "In the code, exploit success probability is defined as:\n",
    "\\begin{equation}\n",
    "q(e) = e^{-weight(e)}\n",
    "\\end{equation}\n",
    "where $weight(e)$ represents the difficulty/complexity of the exploit derived from CVE information.\n",
    "\n",
    "This corresponds to formula (11) in the paper, where $q(e_k)$ represents the probability of success for exploit $k$.\n",
    "\n",
    "#### Basic Explanation\n",
    "This distribution models attacker behavior with periodic defender checks but varying exploit difficulties:\n",
    "\n",
    "- Each edge (exploit) has a difficulty value derived from CVE data\n",
    "- Higher edge weights → lower success probability (more difficult exploit)\n",
    "- Lower edge weights → higher success probability (easier exploit)\n",
    "- Attacker movement depends on sequential success probabilities along a path\n",
    "- defense_rate not used since checks happen at known intervals\n",
    "- attack_rate not needed since progress is determined by exploit difficulties\n",
    "\n",
    "Key insight: Unlike the Poisson model where all steps have equal difficulty, this model accounts for real-world differences in exploit complexity.\n",
    "\n",
    "This fits our intuition: When some exploits are harder than others, attackers must overcome varying challenges along their path, creating natural \"choke points\" that defenders can target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_rate_list = [0]   \n",
    "defense_rate_list = [0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_steps(route, attack_rate=None, defense_rate=None, graph=None):\n",
    "    \"\"\"\n",
    "    Calculates probabilities for attacker movement along route.\n",
    "    Returns probability distribution over possible ending nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Part 1: Extract hardness values from all edges and append them\n",
    "    # into one numpy array\n",
    "    # Calculate hardness values for each edge \n",
    "    hardness = []\n",
    "    for i in range(len(route) - 1):\n",
    "        start_node = route[i]\n",
    "        end_node = route[i + 1]\n",
    "        \n",
    "        # Initialize variables for max weight loop\n",
    "        weights = []\n",
    "        # Collect all weights for max\n",
    "        for edge in graph[start_node][end_node].values():\n",
    "            weights.append(edge.get('weight', DEFAULT_WEIGHT_VALUE))\n",
    "        # Get maximum weight\n",
    "        max_weight = max(weights) if weights else DEFAULT_WEIGHT_VALUE\n",
    "        \n",
    "        # Initialize variables for min weight loop\n",
    "        min_weights = []\n",
    "        # Collect all weights for min\n",
    "        for edge in graph[start_node][end_node].values():\n",
    "            min_weights.append(edge.get('weight', DEFAULT_WEIGHT_VALUE))\n",
    "        # Get minimum weight\n",
    "        min_weight = min(min_weights) if min_weights else DEFAULT_WEIGHT_VALUE\n",
    "            \n",
    "        # Convert weights to probabilities\n",
    "        # We could take max_weight or min_weight here\n",
    "        # hardness.append(np.exp(-max_weight))\n",
    "\n",
    "        # Important: We use min_weight here because of the following reason:\n",
    "        # Since the formula to calculate hardness in R is hardness = exp(-weight)\n",
    "        # taking the minimum weight will give us the maximum hardness\n",
    "        # which translates to the path being EASIEST to traverse.\n",
    "        # Yes hardness of 1 means path is trivial, hardness 0 means path is impossible\n",
    "        hardness.append(np.exp(-min_weight))\n",
    "\n",
    "    \n",
    "    \n",
    "    hardness = np.array(hardness)\n",
    "\n",
    "    # print(f'Hardness: {hardness}')\n",
    "\n",
    "    \n",
    "    ## Part 2: Based on the extracted hardness values\n",
    "    ## Calculate Movement Probabilities\n",
    "\n",
    "    # We calculate two things:\n",
    "    # 1. Probability of reaching each node (accumulating hardness along the way)\n",
    "    # Example: if hardness = [0.8, 0.6, 0.4]\n",
    "    # Then cumprod gives us: [0.8, 0.8*0.6, 0.8*0.6*0.4]\n",
    "    # Final cumulative_probs = [1.0, 0.8, 0.48, 0.192]\n",
    "    cumulative_probs = np.concatenate(([1.0], np.cumprod(hardness)))\n",
    "\n",
    "    # 2. Probability of stopping at each node (based on the next edge's hardness)\n",
    "    stop_probs = np.concatenate((1 - hardness, [1.0]))\n",
    "\n",
    "    ## Part 3: Generate Final Distribution\n",
    "    # Combine reaching and stopping probabilities to get probability of stopping at each node\n",
    "    # Example calculation with above values:\n",
    "    # Node0: 1.0 * 0.2 = 0.2    (20% chance of stopping at start)\n",
    "    # Node1: 0.8 * 0.4 = 0.32   (32% chance of stopping at Node1)\n",
    "    # Node2: 0.48 * 0.6 = 0.288 (28.8% chance of stopping at Node2)\n",
    "    # Node3: 0.192 * 1.0 = 0.192 (19.2% chance of reaching final node)\n",
    "    pdf = cumulative_probs * stop_probs\n",
    "\n",
    "    # Handle case where probabilities are essentially zero\n",
    "    if pdf.sum() < 1e-15:\n",
    "        pdf = np.full_like(pdf, 1e-7)\n",
    "\n",
    "    # Normalize to ensure probabilities sum to 1\n",
    "    # print(f\"This is the final pdf that is returned in the end: {pdf / pdf.sum()}\")\n",
    "    return pdf / pdf.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We start with the baseline graph calculation!\n",
      "Now we are going to run the subgraph analysis\n",
      "The subgraph analysis is done!\n"
     ]
    }
   ],
   "source": [
    "# %run ctr-core_simple.ipynb\n",
    "%run exp_ctr-core.ipynb\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"experiment_3.log\"\n",
      "[1] \"Tue Mar 25 14:31:31 2025\"\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BASELINE RUN: BOTH ATTACKER AND DEFENDER HAVE FULL GRAPH KNOWLEDGE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++\n",
      "\n",
      "The virtual target nodeID is c(12,13,14,16)\n",
      "\n",
      "attack rate =  0 , defense rate =  0 \n",
      "\n",
      "\tequilibrium for multiobjective security game (MOSG)\n",
      "\n",
      "optimal defense strategy:\n",
      "         prob.\n",
      "1 0.000000e+00\n",
      "10 0.000000e+00\n",
      "11 2.720264e-01\n",
      "15 1.852234e-01\n",
      "2 0.000000e+00\n",
      "3 0.000000e+00\n",
      "4 0.000000e+00\n",
      "5 0.000000e+00\n",
      "6 0.000000e+00\n",
      "7 0.000000e+00\n",
      "8 5.427503e-01\n",
      "9 0.000000e+00\n",
      "\n",
      "worst case attack strategies per goal:\n",
      "          1\n",
      "1 0.0000000\n",
      "2 0.1441309\n",
      "3 0.3573620\n",
      "4 0.0000000\n",
      "5 0.0000000\n",
      "6 0.0000000\n",
      "7 0.0000000\n",
      "8 0.0000000\n",
      "9 0.0000000\n",
      "10 0.0000000\n",
      "11 0.4985072\n",
      "[1] 0.089\n",
      "\n",
      "Defender can keep attacker success below: 0.089\n",
      "Attacker can guarantee success probability of: 0.089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(experiment_name+'.log', 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
